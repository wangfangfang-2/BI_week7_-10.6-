Thinking1 什么是反向传播中的链式法则：反向传播：传递输出的误差。反向传播中梯度下降需要每一层都有明确的误差才能更新参数，所以需要通过链式法则将输出层的误差反向传播给隐藏层。误差从输出一层一层往前传播，不可以跳过某些中间步骤，在计算每一步的误差时，需要乘上上一步得到的误差（链式法则，层层相乘）。权重矩阵在反向传播的过程中同样扮演着传递的作用。输出层误差在转置权重矩阵的帮助下，传递到了隐藏层，用来更新与隐藏层相连的权重矩阵.
Thinking2 请列举几种常见的激活函数，激活函数有什么作用：sigmoid激活函数、tanh激活函数、relu激活函数。激活函数的特点是非线性，而数据的分布绝大多数是非线性的，这样可以强化网络的学习能力。不同的激活函数特点不同，应用也不同，sigmoid和tanh函数输出值在(0,1)和(-1,1)之间 => 适合处理概率值。会产生梯度消失 => 不适合深层网络训练，relu的有效导数是常数1，解决了深层网络中出现的梯度消失问题 => 更适合深层网络训练引入非线性函数作为激励函数，这样神经网络表达能力会更加强大 => 不再是输入的线性组合，而是几乎可以逼近任意函数。如果不用激活函数，就相当于激励函数f(x) = x，此时每一层节点的输入都是上层输出的线性函数，那么无论神经网络有多少层，输出都是输入的线性组合 => 与没有隐藏层效果相当。
Thinking3 利用梯度下降法训练神经网络，发现模型loss不变，可能有哪些问题？怎么解决？问题：1.模型结构和特征工程存在问题2.权重初始化方案有问题3.正则化过度4.训练时间不足5.batch size过大6.未进行归一化。解决办法：1.选择合适的激活函数、损失函数1.适当的正则化和降维2.适当降低模型的规模3.不仅仅是初始化，在神经网络的激活函数、损失函数方面的选取，也是需要根据任务类型，选取最合适的。4.选择合适的优化器和学习速率5.神经网络的优化器选取一般选取Adam，但是在有些情况下Adam难以训练，这时候需要使用如SGD之类的其他优化器。